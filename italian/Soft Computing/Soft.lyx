#LyX 2.0 created this file. For more info see http://www.lyx.org/
\lyxformat 413
\begin_document
\begin_header
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language italian
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_amsmath 1
\use_esint 1
\use_mhchem 1
\use_mathdots 1
\cite_engine basic
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Section
Miximum Likelihood Estimation
\end_layout

\begin_layout Standard
Dato un insieme di campioni 
\begin_inset Formula $\theta$
\end_inset

 che sappiamo essere provenienti da un certo tipo di distribuzione, ad esempio
 una gaussiana di varianza nota, calcoliamo la probabilità che i campioni
 appartengano ad una distribuzione generica di quel tipo, ad esempio una
 gaussiana di media incognita.
 
\end_layout

\begin_layout Standard
Tale funzione si dice funzione di likelihood, il parametro (o i parametri)
 che la massimizza determina il modello da adottare.
\end_layout

\begin_layout Subsection
Procedimento
\end_layout

\begin_layout Enumerate
Scrivere likelihood function 
\begin_inset Formula $L=P\left(Data|\theta\right)$
\end_inset


\end_layout

\begin_layout Enumerate
Calcolarne il logaritmo 
\begin_inset Formula $\mathcal{L}=\log P\left(Data|\theta\right)$
\end_inset


\end_layout

\begin_layout Enumerate
Derivare 
\begin_inset Formula $\frac{\partial L}{\partial\theta}$
\end_inset

 o 
\begin_inset Formula $\frac{\partial\mathcal{L}}{\partial\theta}$
\end_inset


\end_layout

\begin_layout Enumerate
Risolvere il sistema di equazioni 
\begin_inset Formula $\frac{\partial\mathcal{L}}{\partial\theta_{i}}=0$
\end_inset


\end_layout

\begin_layout Enumerate
Verificare l'esattezza della soluzione, ovvero che 
\begin_inset Formula $\theta^{mle}$
\end_inset

 massimizza 
\begin_inset Formula $L$
\end_inset

.
\end_layout

\begin_layout Subsection
Gaussiana univariata
\end_layout

\begin_layout Enumerate
\begin_inset Formula $L\left(\mu\right)=\prod_{n=1}^{N}\frac{1}{\sqrt{2\pi\sigma^{2}}}e^{-\frac{\left(x_{n}-\mu\right)^{2}}{2\sigma^{2}}}$
\end_inset


\end_layout

\begin_layout Enumerate
\begin_inset Formula $\mathcal{L}=N\left(\log\frac{1}{\sqrt{2\pi\sigma^{2}}}\right)-\frac{1}{2\sigma^{2}}\sum_{n=1}^{N}\left(x_{n}-\mu\right)^{2}$
\end_inset


\end_layout

\begin_layout Enumerate
\begin_inset Formula $\frac{\partial\mathcal{L}}{\partial\mu}=\frac{1}{2\sigma^{2}}\sum_{n=1}^{N}2\left(x_{n}-\mu\right)$
\end_inset


\end_layout

\begin_layout Enumerate
\begin_inset Formula $\sum_{n=1}^{N}\left(x_{n}-\mu\right)=0$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\mu^{mle}=\frac{1}{N}\sum_{n=1}^{N}x_{n}
\]

\end_inset


\end_layout

\begin_layout Subsection
Metodi numerici: Discesa del gradiente
\end_layout

\begin_layout Enumerate
Prendere random una possibile soluzione 
\begin_inset Formula $k_{0}$
\end_inset


\end_layout

\begin_layout Enumerate
Calcolarne la derivata 
\begin_inset Formula $\frac{\partial E\left(k\right)}{\partial k}|_{k_{n}}$
\end_inset


\end_layout

\begin_layout Enumerate
Aggiornare la soluzione 
\begin_inset Formula $k_{n}=k_{n-1}-\gamma\cdot\frac{\partial E\left(k\right)}{\partial k}|_{k_{n-1}}$
\end_inset


\end_layout

\begin_layout Enumerate
Ripetere i passi 2 e 3 fino a convergenza
\end_layout

\begin_layout Subsubsection
Possibili problemi e possibili soluzioni
\end_layout

\begin_layout Standard
Problemi:
\end_layout

\begin_layout Itemize
Diversi minimi locali.
\end_layout

\begin_layout Itemize
Lenta convergenza.
\end_layout

\begin_layout Itemize
Nessuna convergenza.
\end_layout

\begin_layout Standard
Soluzioni:
\end_layout

\begin_layout Itemize
Multiple inizializzazioni random.
\end_layout

\begin_layout Itemize
Usare derivate seconde.
\end_layout

\begin_layout Itemize
Ridurre 
\begin_inset Formula $\gamma$
\end_inset

.
\end_layout

\begin_layout Section
Neural Networks
\end_layout

\begin_layout Subsection
Introduzione
\end_layout

\begin_layout Subsubsection
Perché
\end_layout

\begin_layout Standard
Caratteristiche di un modello di rete neuronale
\end_layout

\begin_layout Itemize
Gestisce bene dati soggetti a 
\series bold
rumore
\series default
 (intrinseco o ambientale).
\end_layout

\begin_layout Itemize
Fortemente 
\series bold
parallelo
\series default
.
\end_layout

\begin_layout Itemize

\series bold
Ridondante
\series default
, resistente ai guasti.
\end_layout

\begin_layout Itemize
Si 
\series bold
adatta
\series default
 bene a variazioni ambientali.
\end_layout

\begin_layout Standard
Approccio diverso dal paradigma di Von Neumann.
\end_layout

\begin_layout Subsubsection
Nota Linguistica
\end_layout

\begin_layout Description
Rete
\begin_inset space ~
\end_inset

Neurale 
\emph on
Neural Network
\emph default
.
 Rete di neuroni (biologia).
\end_layout

\begin_layout Description
Rete
\begin_inset space ~
\end_inset

Neuronale 
\emph on
Artificiale Neural Network
\emph default
.
 Rete di neuroni artificiali (machine learning).
\end_layout

\begin_layout Subsection
Neurone: dalla biologia al machine learning
\end_layout

\begin_layout Standard
Un neurone accumula carica dai recettori attraverso le sinapsi.
 Possono esserci sinapsi eccitatorie o inibitorie, con pesi variabili.
 Quando questa carica supera una certa soglia, il neurone scarica attraverso
 l'assone.
 
\end_layout

\begin_layout Subsubsection
Neurone artificiale
\end_layout

\begin_layout Standard
\align center
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename img/ArtificialNeuronModel_english.png
	lyxscale 40
	scale 15

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Caption

\begin_layout Plain Layout
Neurone artificiale
\begin_inset CommandInset citation
LatexCommand cite
key "neurone_artificiale"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
Un generico neurone 
\begin_inset Formula $j$
\end_inset

 è descritto da:
\end_layout

\begin_layout Itemize
pesi delle sinapsi 
\begin_inset Formula $w_{j,i}$
\end_inset


\end_layout

\begin_layout Itemize
valore di attivazione 
\begin_inset Formula $net_{j}=\sum_{i}w_{j,i}x_{i}$
\end_inset


\end_layout

\begin_layout Itemize
soglia di attivazione 
\begin_inset Formula $\theta_{j}=-w_{j,0}\cdot1$
\end_inset


\end_layout

\begin_layout Itemize
funzione di attivazione 
\begin_inset Formula $\varphi=g_{j}\left(\right)$
\end_inset


\end_layout

\begin_layout Standard
L'output del neurone è definito come
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
o_{j}=g\left(-\theta_{j}+\sum_{i=1}^{N}w_{j,i}x_{i}\right)=g\left(\sum_{i=0}^{N}w_{j,i}x_{i}\right)
\]

\end_inset


\end_layout

\begin_layout Subsubsection
Comuni funzioni di attivazione
\end_layout

\begin_layout Description
Scalino limitata tra 
\begin_inset Formula $-1$
\end_inset

 e 
\begin_inset Formula $+1$
\end_inset

 con discontinuità di terzo tipo in 
\begin_inset Formula $\left(0,0\right)$
\end_inset

.
\end_layout

\begin_layout Description
Lineare una retta passante per l'origine di arbitrario coefficiente angolare.
\end_layout

\begin_layout Description
Sigmoide limitata tra 
\begin_inset Formula $0$
\end_inset

 e 
\begin_inset Formula $+1$
\end_inset

 sempre derivabile.
\end_layout

\begin_layout Description
Tangente
\begin_inset space ~
\end_inset

iperbolica limitata tra 
\begin_inset Formula $-1$
\end_inset

 e 
\begin_inset Formula $+1$
\end_inset

 sempre derivabile.
\end_layout

\begin_layout Subsection
Percettrone
\end_layout

\begin_layout Standard
È il più semplice modello di neurone artificiale.
\end_layout

\begin_layout Itemize
Singolo strato
\end_layout

\begin_layout Itemize
Modellizza bene le porte logiche
\end_layout

\begin_deeper
\begin_layout Itemize
bisezione del piano di input tramite una retta
\end_layout

\end_deeper
\begin_layout Itemize
Non modellizza lo 
\series bold
XOR
\series default
!
\end_layout

\begin_layout Subsection
Hebbian Learning
\end_layout

\begin_layout Subsubsection
Regola di Hebb 
\end_layout

\begin_layout Standard
Hebb Learning Rule (1949)
\end_layout

\begin_layout Quotation
La forza di una sinapse aumenta in accordo con la simultanea attivazione
 dell'input relativo e dell'obiettivo desiderato.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
w_{i}^{n+1}=w^{n}+\Delta w_{i}
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\Delta w_{i}=\eta\cdot t\cdot x_{i}
\]

\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $\eta$
\end_inset

: learning rate
\end_layout

\begin_layout Itemize
\begin_inset Formula $t$
\end_inset

: output desiderato
\end_layout

\begin_layout Subsubsection
Procedimento di apprendimento
\end_layout

\begin_layout Enumerate
Fissare un tasso di apprendimento 
\begin_inset Formula $\eta$
\end_inset

 arbitrario.
\end_layout

\begin_layout Enumerate
Inizializzare con valori random tutti i pesi 
\begin_inset Formula $w_{i}$
\end_inset

.
\end_layout

\begin_layout Enumerate
Evolvere un'epoca come segue:
\end_layout

\begin_deeper
\begin_layout Enumerate
data una serie di input, calcolare l'output.
\end_layout

\begin_layout Enumerate
aggiornare i pesi secondo la regola di Hebb.
\end_layout

\begin_layout Enumerate
ripetere per tutti i possibili input.
\end_layout

\begin_layout Enumerate
termine dell'epoca.
\end_layout

\end_deeper
\begin_layout Enumerate
Se tutti i 
\begin_inset Formula $\Delta w_{i}$
\end_inset

 erano nulli l'algoritmo è terminato altrimenti ripetere dal punto precedente.
\end_layout

\begin_layout Subsection
Percettrone multistrato
\end_layout

\begin_layout Standard
Risolve il problema dello XOR.
\end_layout

\begin_layout Standard
Oltre a input e output, introduce livelli nascosti di percettroni.
 Questo consente di avere valori intermedi su cui basare l'output.
 A patto di aumentare il numero di percettroni per livello, è sufficiente
 un unico livello nascosto di percettroni per avere l'espressività necessaria
 a modellizzare qualunque rete neuronale con 
\begin_inset Formula $N$
\end_inset

 livelli nascosti.
\end_layout

\begin_layout Description
Layer insieme di neuroni a uguale distanza dagli input.
\end_layout

\begin_layout Description
Input
\begin_inset space ~
\end_inset

Layer layer di neuroni che riceve come input i dati da processare.
\end_layout

\begin_layout Description
Output
\begin_inset space ~
\end_inset

Layer layer di neuroni che forniscono l'output.
\end_layout

\begin_layout Subsubsection
Modello di una rete neuronale
\end_layout

\begin_layout Standard
È non lineare ed è caratterizzato da:
\end_layout

\begin_layout Itemize
numero di neuroni
\end_layout

\begin_layout Itemize
topologia
\end_layout

\begin_layout Itemize
funzioni di attivazione
\end_layout

\begin_layout Itemize
pesi delle sinapsi
\end_layout

\begin_layout Itemize
soglie di attivazione
\end_layout

\begin_layout Standard
\begin_inset Newpage clearpage
\end_inset


\end_layout

\begin_layout Section
Bibliografia
\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "bib/ArtificialNeuronModel_english"
options "plain"

\end_inset


\end_layout

\end_body
\end_document
